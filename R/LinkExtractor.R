#' LinkExtractor
#'
#' Fetch and parse a document by URL, to extract page info, HTML source and links (internal/external).
#' Fetching process can be done by HTTP GET request or through webdriver (phantomjs) which simulate a real browser rendering.
#'
#'
#' @param url character, url to fetch and parse.
#' @param id  numeric, an id to identify a specific web page in a website collection, it's auto-generated byauto-generated by \code{\link{Rcrawler}} function.
#' @param lev numeric, the depth level of the web page, auto-generated by \code{\link{Rcrawler}} function.
#' @param IndexErrPages character vector, http error code-statut that can be processed, by default, it's \code{IndexErrPages<-c(200)} which means only successfull page request should be parsed .Eg, To parse also 404 error pages add, \code{IndexErrPages<-c(200,404)}.
#' @param Useragent , the name the request sender, default to "Rcrawler". but we recommand using a regular browser user-agent to avoid being blocked by some server.
#' @param use_proxy, object created by httr::use_proxy() function, if you want to use a proxy to retreive web page. (does not work with webdriver).
#' @param Timeout ,default to 5s
#' @param urlsZoneXpath, xpath pattern of the section from where links should be exclusively gathered/collected.
#' @param URLlenlimit interger, Maximum URL length to process, default to 255 characters (Useful to avoid spider traps)
#' @param urlregexfilter character vector, filter out extracted internal urls by one or more regular expression.
#' @param urlExtfilter character vector, the list of file extensions to exclude from parsing, Actualy, only html pages are processed(parsed, scraped); To define your own lis use \code{urlExtfilter<-c(ext1,ext2,ext3)}
#' @param ExternalLInks boolean, default FALSE, if set to TRUE external links also are returned.
#' @param urlbotfiler character vector , directories/files restricted by robot.txt
#' @param encod character, web page character encoding
#' @param removeparams character vector, list of url parameters to be removed form web page internal links.
#' @param removeAllparams boolean, IF TRUE the list of scraped urls will have no parameters.
#' @param Browser the client object of a remote headless web driver(virtual browser), created by \code{br<-run_browser()} function, or a logged-in browser session object, created by \link{LoginSession}, after installing web driver Agent \code{install_browser()}. see examples below.
#' @param RenderingDelay the time required by a webpage to be fully rendred, in seconds.
#'
#' @return return a list of three elements, the first is a list containing the web page details (url, encoding-type, content-type, content ... etc), the second is a character-vector containing the list of retreived internal urls and the third is a vetcor of external Urls.
#' @author salim khalil
#'
#' @import webdriver
#' @importFrom  httr GET
#' @importFrom  httr user_agent
#' @importFrom  httr use_proxy
#' @importFrom  httr timeout
#' @importFrom  httr content
#' @importFrom  httr add_headers
#' @importFrom  data.table %chin% %like%
#' @importFrom  xml2  xml_find_all xml_attr xml_text xml_find_first read_html
#'
#' @examples
#'
#'\dontrun{
#'
#' ###### Fetch a URL using GET request :
#' ######################################################
#' ##
#' ## Very Fast, but can't fetch javascript rendred pages or sections
#'
#' # fetch the page with default config, then returns page info and internal links
#'
#' page<-LinkExtractor(url="http://www.glofile.com")
#'
#' # this will return  alse external links
#'
#' page<-LinkExtractor(url="http://www.glofile.com", ExternalLInks = TRUE)
#'
#' # Specify Useragent to overcome bots blocking by some websites rules
#'
#' page<-LinkExtractor(url="http://www.glofile.com", ExternalLInks = TRUE,
#'        Useragent = "Mozilla/5.0 (Windows NT 6.3; Win64; x64)",)
#'
#' # By default, only HTTP succeeded page are parsed, therefore, to force
#' # parse error pages like 404 you need to specify IndexErrPages,
#'
#' page<-LinkExtractor(url="http://www.glofile.com/404notfoundpage",
#'       ExternalLInks = TRUE, IndexErrPages = c(200,404))
#'
#'
#' #### Use GET request with a proxy
#' #
#' proxy<-httr::use_proxy("190.90.100.205",41000)
#' pageinfo<-LinkExtractor(url="http://glofile.com/index.php/2017/06/08/taux-nette-detente/",
#' use_proxy = proxy)
#'
#' #' Note : use_proxy arguments can' not't be configured with webdriver
#'
#' ###### Fetch a URL using a web driver (virtual browser)
#' ######################################################
#' ##
#' ## Slow, because a headless browser called phantomjs will simulate
#' ## a user session on a website. It's useful for web page having important
#' ## javascript rendred sections such as menus.
#' ## We recommend that you first try normal previous request, if the function
#' ## returns a forbidden 403 status code or an empty/incomplete source code body,
#' ## then try to set a normal useragent like
#' ## Useragent = "Mozilla/5.0 (Windows NT 6.3; Win64; x64)",
#' ## if you still have issue then you shoud try to set up a virtual browser.
#'
#' #1 Download and install phantomjs headless browser
#' install_browser()
#'
#' #2 start browser process (takes 30 seconds usualy)
#' br <-run_browser()
#'
#' #3 call the function
#' page<-LinkExtractor(url="http://www.master-maroc.com", Browser = br,
#'       ExternalLInks = TRUE)
#'
#' #4 dont forget to stop the browser at the end of all your work with it
#' stop_browser(br)
#'
#' ###### Fetch a web page that requires authentication
#' #########################################################
#' ## In some case you may need to retreive content from a web page which
#' ## requires authentication via a login page like private forums, platforms..
#' ## In this case you need to run \link{LoginSession} function to establish a
#' ## authenticated browser session; then use \link{LinkExtractor} to fetch
#' ## the URL using the auhenticated session.
#' ## In the example below we will try to fech a private blog post which
#' ## require authentification .
#'
#' If you retreive the page using regular function LinkExtractor or your browser
#' page<-LinkExtractor("http://glofile.com/index.php/2017/06/08/jcdecaux/")
#' The post is not visible because it's private.
#' Now we will try to login to access this post using folowing creditentials
#' username : demo and password : rc@pass@r
#'
#' #1 Download and install phantomjs headless browser (skip if installed)
#' install_browser()
#'
#' #2 start browser process
#' br <-run_browser()
#'
#' #3 create auhenticated session
#' #  see \link{LoginSession} for more details
#'
#'  LS<-LoginSession(Browser = br, LoginURL = 'http://glofile.com/wp-login.php',
#'                 LoginCredentials = c('demo','rc@pass@r'),
#'                 cssLoginFields =c('#user_login', '#user_pass'),
#'                 cssLoginButton='#wp-submit' )
#'
#' #check if login successful
#' LS$session$getTitle()
#' #Or
#' LS$session$getUrl()
#' #Or
#' LS$session$takeScreenshot(file = 'sc.png')
#'
#' #3 Retreive the target private page using the logged-in session
#' page<-LinkExtractor(url='http://glofile.com/index.php/2017/06/08/jcdecaux/',Browser = LS)
#'
#' #4 dont forget to stop the browser at the end of all your work with it
#' stop_browser(LS)
#'
#'
#' ################### Returned Values #####################
#' #########################################################
#'
#' # Returned 'page' variable should include :
#' # 1- list of page details,
#' # 2- Internal links
#' # 3- external links.
#'
#' #1 Vector of extracted internal links  (in-links)
#' page$InternalLinks
#'
#' #2 Vector of extracted external links  (out-links)
#' page$ExternalLinks
#'
#  #3 Page information list
#' page$Info
#'
#' # Requested Url
#' page$Info$Url
#'
#' # Sum of extracted links
#' page$Info$SumLinks
#'
#' # The status code of the HTTP response 200, 401, 300...
#' page$Info$Status_code
#'
#' # The MIME type of this content from HTTP response
#' page$Info$Content_type
#'
#' # Page text encoding UTF8, ISO-8859-1 , ..
#' page$Info$Encoding
#'
#' # Page source code
#' page$Info$Source_page
#'
#' Page title
#' page$Info$Title
#'
#' Other returned values page$Info$Id, page$Info$Crawl_level,
#' page$Info$Crawl_status are only used by Rcrawler funtion.
#'
#'
#' }
#'
#' @export
LinkExtractor <- function(url, id, lev, IndexErrPages, Useragent, Timeout=6, use_proxy=NULL,
                          URLlenlimit=255, urlExtfilter, urlregexfilter, encod, urlbotfiler, removeparams,
                          removeAllparams=FALSE, ExternalLInks=FALSE, urlsZoneXpath=NULL, Browser, RenderingDelay=0) {

  if(!missing(Browser) && !is.null(use_proxy) ) stop("unfortunately, phantomjs can't be configured to use proxy")
  nblinks<-0
  pageinfo<-list()
  links2<- data.frame(href = character(), rel = character(), stringsAsFactors = FALSE)
  linkl<-list() # This will store web elements for links without href in browser mode
  links<- data.frame(href = character(), rel = character(), stringsAsFactors = FALSE)
  Extlinks<- data.frame(href = character(), rel = character(), stringsAsFactors = FALSE)

  if(!missing(Browser)){
    if(length(Browser)<2) stop("please setup a web driver using run_browser()")
  }
  base <- strsplit(gsub("http://|https://", "", url), "/")[[c(1, 1)]]
  if (missing(urlbotfiler)) urlbotfiler<-" "
  if (missing(id)) id<-sample(1:1000, 1)
  if (missing(lev)) lev<-1
  if (missing(IndexErrPages)) errstat<-c(200)
  else errstat<-c(200,IndexErrPages)
  if(missing(Useragent)) Useragent="Mozilla/5.0 (Windows NT 6.3; WOW64; rv:42.0) Firefox/42.0"
  if(missing(urlExtfilter)) urlExtfilter<-c("flv","mov","swf","txt","xml","js","css","zip","gz","rar","7z","tgz","tar","z","gzip","bzip","tar","mp3","mp4","aac","wav","au","wmv","avi","mpg","mpeg","pdf","doc","docx","xls","xlsx","ppt","pptx","jpg","jpeg","png","gif","psd","ico","bmp","odt","ods","odp","odb","odg","odf")
  if (missing(urlregexfilter)){ urlregexfilter<-".*" }
  else { urlregexfilter<-paste(urlregexfilter,collapse="|")}

  if(!missing(Browser)){
    page<-tryCatch(Drv_fetchpage(url = url, browser = Browser), error=function(e) NULL)
    }else {
      if(is.null(use_proxy)){
        page<-tryCatch(httr::GET(url, httr::user_agent(Useragent), httr::timeout(Timeout),httr::add_headers(`Origin`=base)) , error=function(e) list(NULL,e))
      } else {
        page<-tryCatch(httr::GET(url, httr::user_agent(Useragent), use_proxy , httr::timeout(Timeout),httr::add_headers(`Origin`=base)) , error=function(e) list(NULL,e))
      }
    }

    if(length(page)==2){ # Error handling for httr::GET
      if(grepl("Timeout was reached",page[[2]]$message)){
        page<-tryCatch(httr::GET(url, httr::user_agent(Useragent), httr::add_headers(`Origin`=base)) , error=function(e) list(NULL,e))
        if(length(page)==2){
          if(grepl("Timeout was reached",page[[2]]$message)){
              cat ("warning ! Unable to fetch the website using GET request , try to use web driver method (run_browser func see manual)")
          }
          page<-NULL
        }
      } else { # Other errors
        page <- NULL
      }
    }

  if (!is.null(page)){
    if(page$status_code %in% errstat){
      if(grepl("html",page$headers$`content-type`,ignore.case = TRUE)){

        if(missing(Browser) ){ # HTTP GET request based extraction
          html_content <- httr::content(page, as="text", encoding = ifelse(missing(encod), "UTF-8", encod))
          if(is.na(html_content) && missing(encod)){ # Try ISO-8859-1 if UTF-8 failed and no encoding specified
            html_content <- httr::content(page, as="text", encoding = "ISO-8859-1")
          }
          cont <- html_content # Store raw source

          if(!is.na(html_content) && html_content != ""){
            doc <- xml2::read_html(html_content)
            if(!is.null(urlsZoneXpath)){
              for(h in 1:length(urlsZoneXpath)){
                zonex <- tryCatch(xml2::xml_find_all(doc, urlsZoneXpath[[h]]), error=function(e) NULL)
                if(!is.null(zonex)){
                  a_nodes <- xml2::xml_find_all(zonex, ".//a")
                  for (node in a_nodes) {
                    href <- xml2::xml_attr(node, "href")
                    rel <- xml2::xml_attr(node, "rel")
                    if (!is.na(href) && href != "") {
                      links <- rbind(links, data.frame(href = href, rel = ifelse(is.null(rel) || is.na(rel), NA_character_, rel), stringsAsFactors = FALSE))
                    }
                  }
                }
              }
            } else {
                a_nodes <- xml2::xml_find_all(doc, "//a")
                for (node in a_nodes) {
                  href <- xml2::xml_attr(node, "href")
                  rel <- xml2::xml_attr(node, "rel")
                  if (!is.na(href) && href != "") {
                    links <- rbind(links, data.frame(href = href, rel = ifelse(is.null(rel) || is.na(rel), NA_character_, rel), stringsAsFactors = FALSE))
                  }
                }
            }
          }
        } else { # Browser based extraction (e.g. PhantomJS)
          Sys.sleep(RenderingDelay)
          cont <- page$PageSource # Store raw source from browser

          if(!is.null(urlsZoneXpath)){
            w<-1
            for(h in 1:length(urlsZoneXpath)){
              zonex_elements <- tryCatch(Browser$session$findElements(xpath = urlsZoneXpath[[h]]), error=function(e) NULL)
              if(!is.null(zonex_elements)){
                for(zonex_element in zonex_elements){ # If multiple zones match
                    link_elements <- tryCatch(zonex_element$findElements(xpath = ".//a" ), error=function(e) NULL)
                    if(!is.null(link_elements)){
                        for(l_el in link_elements){
                            href <- tryCatch(unlist(l_el$getAttribute("href")), error = function(e) NULL)
                            rel <- tryCatch(unlist(l_el$getAttribute("rel")), error = function(e) NULL)
                            if (!is.null(href) && !is.na(href) && href != "") {
                                links <- rbind(links, data.frame(href = href, rel = ifelse(is.null(rel) || is.na(rel), NA_character_, rel), stringsAsFactors = FALSE))
                            } else {
                                linkl[[w]] <- l_el
                                w <- w+1
                            }
                        }
                    }
                }
              }
            }
          } else {
            link_elements <- tryCatch(Browser$session$findElements(xpath = "//a"), error=function(e) NULL)
            w<-1
            if(!is.null(link_elements)){
                for(l_el in link_elements){
                    href <- tryCatch(unlist(l_el$getAttribute("href")), error = function(e) NULL)
                    rel <- tryCatch(unlist(l_el$getAttribute("rel")), error = function(e) NULL)
                    if (!is.null(href) && !is.na(href) && href != "") {
                        links <- rbind(links, data.frame(href = href, rel = ifelse(is.null(rel) || is.na(rel), NA_character_, rel), stringsAsFactors = FALSE))
                    } else {
                        linkl[[w]] <- l_el
                        w <- w+1
                    }
                }
            }
          }
        }

        # Deduplicate links based on href
        if (nrow(links) > 0) {
          links <- links[!duplicated(links$href), ]
        }

        domain0<- strsplit(gsub("http://|https://|www\\.", "", url), "/")[[c(1, 1)]]
        domain<- paste(domain0, "/", sep="")

        if (nrow(links) > 0) {
          # Link canonicalization
          links$href <- LinkNormalization(links$href,url)

          # Ignore some Url parameters or remove all parameters
          if (!missing(removeparams)){
            if(removeparams!=""){
              links$href <-sapply(links$href , function(x) Linkparamsfilter(x, removeparams), USE.NAMES = FALSE)
            }
          }
          if (removeAllparams){
            links$href <-sapply(links$href , function(x) Linkparamsfilter(x, removeAllparams = TRUE), USE.NAMES = FALSE)
          }
          # Deduplicate again after normalization and param filtering
          links <- links[!duplicated(links$href), ]
        }

        # Link robots.txt filter
        if (!missing(urlbotfiler) && nrow(links) > 0){
          links <- links[!links$href %like% paste(urlbotfiler,collapse="|"), ]
        }

        if(nrow(links) > 0) {
          for(s in 1:nrow(links)){
            current_link_href <- links$href[s]
            current_link_rel <- links$rel[s]

            if (!is.na(current_link_href)){
              if( nchar(current_link_href) <= URLlenlimit) {
                ext<-tools::file_ext(sub("\\?.+", "", basename(current_link_href)))
                if(grepl(domain,current_link_href) && !(current_link_href %in% links2$href) && !(ext %in% urlExtfilter) && grepl(pattern = urlregexfilter,x = current_link_href, perl = TRUE)){
                  links2 <- rbind(links2, data.frame(href = current_link_href, rel = current_link_rel, stringsAsFactors = FALSE))
                  nblinks<-nblinks+1
                }
                 if(ExternalLInks){
                   if ( !grepl(domain,current_link_href) && !(current_link_href %in% Extlinks$href) && !(ext %in% urlExtfilter)){
                      Extlinks <- rbind(Extlinks, data.frame(href = current_link_href, rel = current_link_rel, stringsAsFactors = FALSE))
                      nblinks<-nblinks+1
                   }
                 }
              }
            }
          }
        }
      } else { # Not HTML content
        cont<-"NULL"
      }
    } else { # HTTP status not in errstat
      cont<-"NULL" # Or handle as error page
    }

    titre <- "NULL"
    if(!is.null(cont) && cont != "NULL" && grepl("html",page$headers$`content-type`,ignore.case = TRUE)){
        doc_for_title <- tryCatch(xml2::read_html(cont), error=function(e) NULL)
        if(!is.null(doc_for_title)){
            titre_node <- xml2::xml_find_first(doc_for_title, "//title")
            if(!is.null(titre_node)) titre <- xml2::xml_text(titre_node)
        }
    }

    contenttype<-tryCatch(gsub("(.*)\\;.*", "\\1", page$headers$`content-type`), error=function(e) "NA")
    if(page$headers$`content-type`=="html"){ #This check might be redundant if already checked above
      contentencod<-GetEncodingHTML(cont) # Assuming GetEncodingHTML can handle NULL or non-HTML cont
    } else{
      contentencod<-tryCatch(gsub("(.*)=(.*)","\\2", gsub(".*\\;.", "\\1", page$headers$`content-type`)), error=function(e) "NA")
    }
    pageinfo<-list(Id=id,Url=url,Crawl_status="finished",Crawl_level=lev,SumLinks=nblinks,"", Status_code=page$status_code, Content_type=contenttype, Encoding=contentencod, Source_page=cont, Title=titre)
  } else { # Page is NULL (fetch failed)
    pageinfo<-list(Id=id,Url=url,Crawl_status="NULL",Crawl_level=lev,SumLinks=0,Status_code="N/A",Content_type="N/A",Encoding="N/A",Source_page="N/A",Title="N/A")
  }

  # Ensure Extlinks is an empty data.frame if not used
  if(!ExternalLInks && nrow(Extlinks) == 0){
      Extlinks <- data.frame(href = character(), rel = character(), stringsAsFactors = FALSE)
  }

  paquet<-list(Info=pageinfo,
             InternalLinks=links2, # Should be a data.frame
             ExternalLinks=Extlinks) # Should be a data.frame
  if(!missing(Browser)){
    paquet$OtherLinksTags <- linkl # List of web elements without href
  }

  return(paquet)
}


#' Fetch page using web driver/Session
#'
#' @param url character, web page URL to retreive
#' @param browser Object returned by \code{\link{run_browser}}
#'
#' @return return a list of three elements, the first is a list containing the web page details (url, encoding-type, content-type, content ... etc), the second is a character-vector containing the list of retreived internal urls and the third is a vetcor of external Urls.
#' @author salim khalil
#'
#' @import  webdriver
#' @importFrom  jsonlite fromJSON
#' @importFrom  jsonlite validate

#' @export

Drv_fetchpage <- function(url, browser) {

  if (missing(browser)) stop("browser argument is missing! use run_browser() to build a browser object or LoginSession() for pages requiring authentification")
  if (missing(url)) stop("url argument is missing! you need to provide the url to be fetched")

  if (length(browser)<3){ # Assuming browser object has at least 3 elements if session initialized
      tryCatch(browser$session$initialize(port=browser$process$port), error = function(e) {
          stop(paste("Failed to initialize browser session:", e$message))
      })
  }
  tryCatch(browser$session$go(url), error = function(e) {
      stop(paste("Failed to navigate to URL:", url, "-", e$message))
  })

  # one login  try
  if (length(browser)==3){ # This condition seems specific, might need review for robustness
    if(grepl(browser$loginInfo$LoginURL,browser$session$getUrl())){
      LoginSession(Browser = browser, LoginURL = browser$loginInfo$LoginURL, LoginCredentials = browser$loginInfo$LoginCredentials,
                   cssLoginFields = browser$loginInfo$cssLoginFields, cssLoginButton = browser$loginInfo$cssLoginButton,cssRadioToCheck = browser$loginInfo$cssRadioToCheck,
                   XpathLoginFields = browser$loginInfo$XpathLoginFields, XpathLoginButton = browser$loginInfo$XpathLoginButton, browser$loginInfo$XpathRadioToCheck
                    ) # Removed redundant browser$loginInfo$XpathRadioToCheck
      browser$session$go(url)
    }
  }

  sc=as.character(browser$session$getSource())
  # HAR log fetching can be problematic and resource-intensive, ensure it's needed or handle errors
  x <- tryCatch(browser$session$readLog(type = "har"), error = function(e) {
      warning("Failed to read HAR log from browser session. Proceeding without HAR data.")
      return(NULL) # Return NULL if HAR log fails
  })

  xjson <- NULL
  if (!is.null(x) && !is.null(x$message) && jsonlite::validate(x$message)) {
    xjson <- tryCatch(jsonlite::fromJSON(x$message, simplifyVector = FALSE), error = function(e) NULL)
  }

  status_c <- 200 # Default status
  content_type <- "text/html" # Default content type

  if(!is.null(xjson) && length(xjson)>0 && !is.null(xjson$log$entries) && length(xjson$log$entries)>0){
      # Corrected logic to find the specific entry for the main URL
      target_entry <- NULL
      url_variants <- c(url, if(substring(url, nchar(url)) == "/") substr(url, 1, nchar(url)-1) else paste0(url,"/"))

      for (entry in xjson$log$entries) {
          if (entry$request$url %in% url_variants) {
              target_entry <- entry
              break
          }
      }
      # Fallback: try regex if exact match failed (e.g. due to redirects not captured as main entry)
      if(is.null(target_entry)){
          for (entry in xjson$log$entries) {
              if (grepl(paste0("^",gsub("([.|()\\^{}+$*?]|\\[|\\])", "\\\\\\1", url),"(\\/)?(#.*)?$"), entry$request$url)) {
                 target_entry <- entry
                 break
              }
          }
      }

      if(!is.null(target_entry)){
        if("status" %in% names(target_entry$response)){
          status_c<-target_entry$response$status
        }
        if(length(target_entry$response$headers)>0){
          found_content_type <- FALSE
          for(i in 1:length(target_entry$response$headers)){
            if("name" %in% names(target_entry$response$headers[[i]])){
              if (tolower(target_entry$response$headers[[i]]$name) == "content-type"){ # Case-insensitive match
                content_type<-target_entry$response$headers[[i]]$value
                found_content_type <- TRUE
                break
              }
            }
          }
          if(!found_content_type) content_type <- get_contenttype(sc) # Fallback if not in HAR
        } else {
            content_type <- get_contenttype(sc) # Fallback if no headers in HAR
        }
      } else { # If no matching entry in HAR
          content_type <- get_contenttype(sc) # Fallback
      }
  } else { # If HAR log is empty or not parsable
    content_type <- get_contenttype(sc) # Fallback
  }

  page_data<-list(status_code=status_c,
             PageSource=sc,
             headers=list(`content-type`=content_type))

  return(page_data)
}

# Helper function to guess content type from source if not in headers
get_contenttype<- function (sc){
  content_type<-""
  if(is.null(sc) || sc == "") return("")
  if(grepl('^<\\?xml', trimws(sc))) content_type<-"application/xml" # More specific XML MIME type
  else if ( grepl("<!doctype.*html",trimws(sc), perl=TRUE,ignore.case = TRUE) # Stricter HTML check
            || grepl("<html",trimws(sc), perl=TRUE,ignore.case = TRUE))
    content_type<-"text/html"
  else if(jsonlite::validate(sc)) content_type<-"application/json" # More specific JSON MIME type
  else content_type<-"application/octet-stream" # Generic fallback
  return (content_type)
}
